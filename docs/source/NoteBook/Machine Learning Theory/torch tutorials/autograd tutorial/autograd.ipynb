{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Autograd\n",
    "\n",
    "torch.Autograd is a python module whose purpose is to calculate the gradient of functions using the backpropagation method. \n",
    "\n",
    "Backpropagation is an algorithm relying on the following theorem :\n",
    "$$ (f(g(x)))' = g'(x) * f'(g(x)) $$\n",
    "\n",
    "It means that to compute $(f(g(x)))'$ we only need to know $f',g(x)$ and $g'(x) $. \n",
    "It the context of Neural Networks it is useful because they are compostions of simple function such as activation functions and linear/affine functions, so we know $f'$ and by a reccursive call we can have $g(x)$ and $g'(x)$ : this is the backpropagation algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The job of Autograd is to keep in mind which function depends on which functions to perform backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computationnal graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we need to compute the gradient of a function relying on a tensor t, then we need to \n",
    "define t with the parameter requires_grad = True. Let's try to minimise the function $2*(x^2 + y^2)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(3,dtype=torch.float,requires_grad=True)\n",
    "y = torch.tensor(2,dtype=torch.float,requires_grad=True)\n",
    "z = x**2+y**2\n",
    "t = 2*z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note :\n",
    "- $S : (x,y) \\rightarrow x+y$ \n",
    "- $P : x \\rightarrow x^2$\n",
    "- $M : x \\rightarrow 2x$.\n",
    "\n",
    "Then we have : \n",
    "$$\n",
    "\\begin{cases}\n",
    "u = P(x) \\\\\n",
    "v= P(y) \\\\\n",
    "z = S(u,v) \\\\\n",
    "t = M(z) = M(z(u,v)) = M(z(u(x),u(y)))\n",
    "\\end{cases}\n",
    "$$\n",
    "and \n",
    "$$\n",
    "\\begin{cases}\n",
    "x = 3\\\\\n",
    "y = 2 \\\\\n",
    "u = 9 \\\\\n",
    "v = 4\n",
    "z = 13 \\\\\n",
    "t = 26\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "To minimise the function we propose to use the gradient descent algorithm. To do that we need to \n",
    "compute the gradient of $ M(S(P(x),P(y)))$ with respect to the parameters $x,y$. We do the backpropagation method : \n",
    "1) we compute the gradient of $t$ with respect to $z$ at the point $z=13$ : its $2$\n",
    "2) we compute the gradient of $t$ with respect to $u$ and $v$ at point (u=9,v=4) : \n",
    "    $$\n",
    "        \\frac{\\partial t(9,4)}{\\partial u} = \\frac{\\partial M(13)}{\\partial z} \\cdot \\frac{\\partial z(9,4)}{\\partial u} = 2 * 1 $$\n",
    "    And we have the same result if we do the computation for $v$\n",
    "3) We compute the gradient of $t$ with respect to $x$ and $y$ :\n",
    "    $$ t(x,y) = M(S(P(x),P(y))) $$\n",
    "    $$ \n",
    "        \\frac{\\partial t(3,2)}{\\partial x}  \\\\\n",
    "        = \\frac{\\partial (MoS)(9,4)}{\\partial u} \\cdot  \\frac{\\partial u(3)}{\\partial x} + \\frac{\\partial (MoS)(9,4)}{\\partial v} \\cdot  \\frac{\\partial v(3)}{\\partial x} \\\\\n",
    "        = \\frac{\\partial (MoS)(9,4)}{\\partial u} \\cdot  \\frac{\\partial u(3)}{\\partial x} + 0\n",
    "        = \n",
    "    $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rj/jncxshgs6tjb572dxm8jzvwc0000gn/T/ipykernel_58507/4057985052.py:6: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:494.)\n",
      "  print(z.grad)\n"
     ]
    }
   ],
   "source": [
    "t = torch.ones((2,2),dtype=torch.float,requires_grad=True)\n",
    "z = 2*t \n",
    "z = -t \n",
    "u = z.sum()\n",
    "u.backward()\n",
    "print(z.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
