{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2f637c2",
   "metadata": {},
   "source": [
    "# *Introduction to few key NLP metrics*\n",
    "\n",
    "In any scenario, where there is a need to judge how good a machine translation of a given source language is, or judge the performance of a classifier, we need to introduce metrics. Note that, in case of machine translation source and target language can be anything--a PDF, latex code corresponding to a PDF, any human language, etc. Hence these metrics are language-independent. We attempt to introduce few of these metrics here:\n",
    "\n",
    "+ Levenshtein distance (Edit distance)\n",
    "+ BLEU (BiLingual Evaluation Understudy)\n",
    "+ ROUGE (\\# Todo)\n",
    "+ METEOR(Metric for Evaluation of Translation with Explicit ORdering) (\\# Todo)\n",
    "+ Precision\n",
    "+ Accuracy\n",
    "+ Recall\n",
    "+ F1 Score\n",
    "\n",
    "## Machine Translations related\n",
    "\n",
    "### Levenshtein distance\n",
    " Levenshtein or Edit distance is a fundamental string metric. Given two strings $A$ and $B$, it computes minimum number of insertions, deletions or replacements needed to transform $A$ to $B$. It helps to compute how much different a machine translated string is from the source string. Edit distance has a convenient recursive formula:\n",
    " \n",
    " $$lev_{a,b}(i,j) = \\begin{cases} \n",
    "          lev_{a,b}(i+1,j+1) & if \\ a[i]=b[j], a,b\\neq\\phi \\\\\n",
    "          1+min\\{lev_{a,b}(i,j+1),lev_{a,b}(i+1,j),lev_{a,b}(i+1,j+1)\\} & if \\ a[i] \\neq b[j], a,b\\neq \\phi \\\\\n",
    "          |a[i:]| & if \\ b=\\phi \\\\\n",
    "          |b[j:]| & if \\ b\\neq\\phi, a=\\phi\n",
    "       \\end{cases}$$\n",
    "       \n",
    "where <mark>$lev_{a,b}(i,j)$ is the levenshtein distance of the string $a$ starting from $i^{th}$ index w.r.t. string $b$ starting from $j^{th}$ index</mark>. \n",
    "\n",
    "*EXPLANATION*: In order to compute $lev_{a,b}(i,j)$:\n",
    "+ If $a[i] = b[j]$, then it is quite clear that we recursively compute the Edit distance of the string $a[i+1:]$ w.r.t. $b[j+1:]\n",
    " $\n",
    "+ If $a[i] \\neq b[j]$, then either:\n",
    "  + we add $b[j]$ in the current position of $a$, so we recursively compute the Edit distance of the string $a[i:]$ w.r.t. $b[j+1:]$ (as $b[j]$ has been matched but didn't exhaust any character of the original $a$) and add $1$ to result(for the $Add$ operation).\n",
    "  + we replace $a[i]$ with $b[j]$, so we recursively compute the Edit distance of the string $a[i+1:]$ w.r.t. $b[j+1:]$ (since $b[j]$ has been matched, and exhausted one more character of $a$) and add $1$ to result(for the $Replace$ operation).\n",
    "  + we delete $a[i]$ in hope that the rest of the $a$ string matches with rest of the $b$ string, so we recursively compute Edit distance of $a[i+1:]$ w.r.t. $b[j]$(since character of $a$ has been exhausted without matching $b[j]$)\n",
    " \n",
    "The recursive formula, naturally leads to a bottom up dynamic programming approach, where we take a $(|a|+1)$x$(|b|+1)$ matrix $dp$ with rows indexed by $a$ and column by $b$ such that (we output $dp[0][0]=lev_{a,b}(0,0)$ at the end):\n",
    "$$ dp[i][j] =\\begin{cases}\n",
    "lev_{a,b}(i,j) & if \\ 0\\leq i\\leq|a|, 0\\leq j\\leq|b|\\\\\n",
    "lev_{a,\\phi}(i,j) & if \\ j=|b| \\\\\n",
    "leb_{\\phi,b}(i,j) & if \\ i=|a|\\\\\n",
    "\\end{cases}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ce0e55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs in O(len(str1)*len(str2))\n",
    "def levenshtein( str1, str2):\n",
    "    l1 = len(str1)\n",
    "    l2 = len(str2)\n",
    "    matrix = [[-1]*(l2+1) for i in range(0,l1+1)] # initialisation of the (len(str1)+1)x(len(str2)+1) matrix with -1(s)\n",
    "    \n",
    "    # base cases\n",
    "    for i in range(l2+1):\n",
    "        matrix[l1][i] = l2 - i # initialisation of the bottom row\n",
    "    \n",
    "    for j in range(l1+1):\n",
    "        matrix[j][l2] = l1 - j # initialisation of the rightmost column\n",
    "        \n",
    "    # computation starts!\n",
    "    for i in range(l1-1,-1,-1): # bottom-up fashion--- from second last row to top row, and in each row from second-last cell to starting cell\n",
    "        for j in range(l2-1,-1,-1):\n",
    "            if str1[i]==str2[j]:\n",
    "                matrix[i][j] = matrix[i+1][j+1] # if the characters are equal, move both pointers\n",
    "            else:\n",
    "                matrix[i][j] = 1 + min(matrix[i+1][j],matrix[i+1][j+1], matrix[i][j+1]) # else, add 1 (pertaining to either insert, replace or delete) to the minimum of the levenshstein distances of the remaining pieces of the strings\n",
    "            \n",
    "    \n",
    "    return matrix[0][0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b193c0",
   "metadata": {},
   "source": [
    "Examples:\n",
    "+ levenshtein distance of *\"abc\"* w.r.t *\"adc\"* is $1$ : *\"abc\"*  $\\xrightarrow[]{\\text{replace \"b\" with \"d\"}}$ *\"adc\"*\n",
    "+ levenshtein distance of *\"horse\"* w.r.t. *\"ros\"* is $3$ : *\"horse\"* $\\xrightarrow[]{\\text{replace \"h\" with \"r\"}}$ *\"rorse\"* $\\xrightarrow[]{\\text{delete \"r\"}}$ *\"rose\"* $\\xrightarrow[]{\\text{delete \"e\"}}$ *\"ros\"* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db9f4f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(levenshtein(\"abc\", \"adc\"))\n",
    "print(levenshtein(\"horse\",\"ros\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04dc4e9b",
   "metadata": {},
   "source": [
    "### BLEU metric\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4c4b61",
   "metadata": {},
   "source": [
    "<mark>Definition 1 (*$n-gram$*)</mark>: Generally an $n-gram$ is a sequence of $n$ consecutive words, letters, syllables or base pairs (*for machine translation(MT) purposes they are generally words*) collected from a text corpus(dataset).\n",
    "\n",
    "Ex: bigrams($2$-grams) of *\"This article is on NLP\"* are *\"This article\"*, *\"article is\"*, *\"on NLP\"*, etc. \n",
    "\n",
    "Given an Machine Translation of a source \"sentence\", and one or more \"references\", we need to able to say how close the translaton is w.r.t the references and clearly distinguish a good candidate from a bad one.\n",
    "\n",
    "<mark>Definition 2(*modified $n-gram$ precision*)</mark> Modified $n-gram$ precision of a candidate sentence w.r.t. one or more references is \n",
    "\n",
    "$$ \\frac{\\sum\\limits_{\\mathcal{C}\\in unique \\  n-grams} \\ count\\_clip(\\mathcal{C})}{number \\ of \\ n-grams}$$, \n",
    "\n",
    "where \n",
    "$count\\_clip(\\epsilon) = min(Count, Max\\_Ref\\_Count)$ , $Count$=number of times the $n-gram$ $\\epsilon$ appears in the candidate sentence, $Max\\_Ref\\_Count$ is the maximum Count of $\\epsilon$ over all reference(sentences).\n",
    "\n",
    "We can calculate the modified $n-gram$ precision using `nltk.translate.bleu_score.modified_precision` method of the NLTK library. (install NLTK using `pip install nltk`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac18e215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unigrams = 44\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8409090909090909"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "# example 1\n",
    "\n",
    "\n",
    "reference  = '\\dfrac{1}{\\sqrt{n} \\Sigma_{i=1}^{n} |i \\rangle'\n",
    "ref        = [char for char in reference]\n",
    "\n",
    "candidate1 = '\\frac{1}{\\sqrt{n} \\Sigma\\limits_{i=1}^{n} |i>'\n",
    "cand       = [char for char in candidate1]\n",
    "print(\"Number of unigrams =\",len(cand))\n",
    "references = [ref]\n",
    "float(nltk.translate.bleu_score.modified_precision(references, cand, n=1)) # modified 1-gram precision = 37/44\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "18f23b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07692307692307693\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5714285714285714"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example 2\n",
    "\n",
    "reference1 = 'It is a guide to action that ensures that the military will forever heed Party commands'.split()\n",
    "reference2 = 'It is the guiding principle which gurantees the military forces always being under the command of the Party'.split()\n",
    "reference3 = 'It is the practical guide for the army always to heed the directions of the party'.split()\n",
    "\n",
    "candidate1 = 'It is to insure the troops forever hearing the activity guidebook that party direct'.split()\n",
    "references = [reference1, reference2, reference3]\n",
    "print(float(nltk.translate.bleu_score.modified_precision(references, candidate1, n=2))) # modified 2-gram precision = 1/13\n",
    "float(nltk.translate.bleu_score.modified_precision(references, candidate1, n=1)) # modified 1-gram precision = 8/14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c765c2d",
   "metadata": {},
   "source": [
    "Note: Since we calculate modified $n-gram$ precision for a \"sentence\"(for  $\\LaTeX$ code, it might refer to a single sequence of tokens representing the candidate $\\LaTeX$ code for an equation, an image, a table, etc), in order to calculate $n-gram$ precision (denoted $p_n$ )for the whole document(collection of \"sentences\") we use the following formula:\n",
    "$$ p_n = \\frac{\\sum\\limits_{\\mathcal{C}\\in \\{ Candidates \\}}\\sum\\limits_{n-gram \\in \\mathcal{C}}count\\_clip(\\mathcal{n-gram})}{\\sum\\limits_{\\mathcal{C'}\\in \\{ Candidates \\}}\\sum\\limits_{n-gram \\in \\mathcal{C'}}count(\\mathcal{n-gram})} $$.\n",
    "\n",
    "<table><tr><th>Why should we consider a weighted average of $n-gram$ precisions over multiple $n$?</th></tr></table>\n",
    "It is obvious as $n$-increases(upto a certain point) modified $n-gram$ precision of a corpus(whole document) better relates to translation quality:\n",
    "\n",
    "+ *low $n$*: Checks the vocabulary of tokens/words used in the candidate, i.e. ensures the candidate doesn't use too many unrelated tokens, but it doesn't enforce token order and one might have a complete garbage candidate with correct tokens all jumbled up.\n",
    "\n",
    "+ *high $n$*: Checks and ensures proper token order and longer syntactic rules, but may be too harsh on minor deviations from \"desired\" references.\n",
    "\n",
    "Hence, we need to average out the modified precision scores for all $n\\leq N$(threshold).\n",
    "\n",
    "But...\n",
    "<figure>\n",
    "    <img src=\"image1.jpg\" alt=\"Alt text\" />\n",
    "    <figcaption><center><mark>Exponentially decaying $n-gram$ scores. Here $H_{i}:Human_{i} $,$S_{j}:System/Machine_{j}$</mark></center></figcaption>\n",
    "</figure>\n",
    "\n",
    "as the image shows, modified $n$-gram scores decays rough exponentially with $n$ for human translators(\"good\") as well as machine translators(\"bad\").\n",
    "\n",
    "Therefore we use a heuristic wherein we take <mark>weighted average of logarithms of $p_n$</mark> for $n\\leq N$. (The baseline BLEU uses uniform weights)\n",
    "\n",
    "<mark>Sentence length</mark>: A candidate should neither be too short, nor too long, already the modified $n-gram$ precisions punish longer candidates that repeat a particular instance of a token too much, or use spurious words, **but**, too short candidates using the right words and orders(ex: a fragment of a reference) are spared, as seen in the following example.  Therefore we deploy a *brevity penalty* on longer sentences.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae84ee5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# example\n",
    "reference1 = 'It is a guide to action that ensures that the military will forever heed Party commands'.split()\n",
    "reference2 = 'It is the guiding principle which gurantees the military forces always being under the command of the Party'.split()\n",
    "reference3 = 'It is the practical guide for the army always to heed the directions of the party'.split()\n",
    "\n",
    "candidate1 = 'of the'.split() # very short and undesirable candidate\n",
    "references = [reference1, reference2, reference3]\n",
    "for i in range(1,3):\n",
    "    print(float(nltk.translate.bleu_score.modified_precision(references, candidate1, n=i))) # score is still 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8060ae16",
   "metadata": {},
   "source": [
    "<mark>Definition $3$(*Brevity penalty*)</mark>: Let $r_s(\\mathcal{C}) = |\\epsilon| s.t. ||\\epsilon|-|\\mathcal{C}||=\\min\\limits_{\\mathcal{R}\\in \\{References\\}}||\\mathcal{C}|-|\\mathcal{R}||$. Call $r_s$ the *best match length*.\n",
    "Let $r$(*effective refernce corpus length*) be the sum of *best match lengths*($r_s$) of each sentence in the corpus, then $$BP(\\text{Brevity Penalty}) =\\begin{cases}\n",
    "1 & if \\ c > r\\\\\n",
    "e^{1-\\frac{r}{c}} & if \\ c \\leq  r\n",
    "\\end{cases}$$, where $c=|\\mathcal{C}|$\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f97cb4c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0009118819655545162"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference1 = 'It is a guide to action that ensures that the military will forever heed Party commands'.split()\n",
    "reference2 = 'It is the guiding principle which gurantees the military forces always being under the command of the Party'.split()\n",
    "reference3 = 'It is the practical guide for the army always to heed the directions of the party'.split()\n",
    "\n",
    "candidate1 = 'of the'.split() # very short and undesirable candidate\n",
    "references = [reference1, reference2, reference3]\n",
    "cand_len = len(candidate1)\n",
    "closest_ref_len =  nltk.translate.bleu_score.closest_ref_length(references, cand_len)\n",
    "nltk.translate.bleu_score.brevity_penalty(closest_ref_len, cand_len) # brevity penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98922736",
   "metadata": {},
   "source": [
    "Final BLEU Metric:\n",
    "\n",
    "$$BLEU = BP. \\exp{(\\sum\\limits_{n=1}^{N}w_n \\log{p_n})}$$\n",
    "\n",
    "In the baseline metric, $w_n=\\frac{1}{N}$ and $N=4$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43d16562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7447490192819548"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example\n",
    "reference  = '\\dfrac{1}{\\sqrt{n} \\Sigma_{i=1}^{n} |i \\rangle'\n",
    "ref        = [char for char in reference]\n",
    "\n",
    "candidate1 = '\\frac{1}{\\sqrt{n} \\Sigma\\limits_{i=1}^{n} |i>'\n",
    "cand       = [char for char in candidate1]\n",
    "list_of_references = [[ref]]\n",
    "hypotheses = [cand]\n",
    "\n",
    "nltk.translate.bleu_score.corpus_bleu(list_of_references, hypotheses) # BLEU score, by default uniform weight and N=4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b096102b",
   "metadata": {},
   "source": [
    "### METEOR metric\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ff472c",
   "metadata": {},
   "source": [
    "Sort of a more robust BLEU. Allows synonyms and stemmed words to be matched with the reference word.\n",
    "\n",
    "\\# Todo: Brief description with examples with related code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b736126",
   "metadata": {},
   "source": [
    "## Classification related\n",
    "### Accuracy\n",
    "\n",
    "<mark> Definition (*Confusion Matrix*)</mark>:The columns of the matrix represent the instances of the predicted classes predicted by a classifier and the rows represent the instances of the actual class. It is used to visualize the performance of a classifier. Note that the role of rows and columns can be inverted in some representations.\n",
    "\n",
    "Let us work with a binary classifier for now, with two classes namely **Positive** and **Negative**.\n",
    "\n",
    "\\begin{array}{|c|c|c|}\n",
    "\\hline\n",
    " & \\textbf{Predicted Negative} & \\textbf{Predicted Positive} \\\\\n",
    "\\hline\n",
    "\\textbf{Actual Negative} & TN & FP \\\\\n",
    "\\hline\n",
    "\\textbf{Actual Positive} & FN & TP \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "\n",
    "where $TN: True \\ Negative$, $TP: True \\ Positive$, $FN:False \\ Negative$, $FP: False \\ Positive$.\n",
    "\n",
    "Then *Accuracy* is then defined as : $ = {{TP + TN} \\over {TP + TN + FP + FN}}$\n",
    "The following is a logistic regression model which uses artificial train and test set. <mark>You can ignore all the code and simply look at the confusion matrix to calculate accuracy.</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd1e5e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "              Predicted Negative   Predicted Positive\n",
      "Actual Negative      15                  1\n",
      "Actual Positive      0                  14\n",
      "\n",
      "Accuracy of the classifier:\n",
      "Accuracy = 96.67%\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "# Generate a synthetic binary classification dataset\n",
    "X, y = make_classification(n_samples=100, n_features=2, n_informative=2, \n",
    "                            n_redundant=0, random_state=42, n_classes=2)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a Logistic Regression model\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "tn, fp, fn, tp = conf_matrix.ravel()  # Extract values from the confusion matrix\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print results\n",
    "print(\"Confusion Matrix:\")\n",
    "print(f\"              Predicted Negative   Predicted Positive\")\n",
    "print(f\"Actual Negative      {tn}                  {fp}\")\n",
    "print(f\"Actual Positive      {fn}                  {tp}\")\n",
    "\n",
    "print(\"\\nAccuracy of the classifier:\")\n",
    "print(f\"Accuracy = {accuracy:.2%}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e18ce45",
   "metadata": {},
   "source": [
    "**From now on, we will cook up arbitary confusion matrices, just to illustrate the metric, without actually spawning the classifier!**\n",
    "\n",
    "Note: Accuracy alone is not a sufficient metric of performance as shown by the following example:\n",
    "\n",
    "\\begin{array}{|c|c|c|}\n",
    "\\hline\n",
    " & \\textbf{Predicted Negative} & \\textbf{Predicted Positive} \\\\\n",
    "\\hline\n",
    "\\textbf{Actual Negative} & 0 & 5 \\\\\n",
    "\\hline\n",
    "\\textbf{Actual Positive} & 0 & 95 \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "\n",
    "The classfier can have $0.95$ accuracy predicting \"*Positive*\" always!\n",
    "\n",
    "### Precision\n",
    "<mark>Definition</mark>: Fraction of True positives to all predicted positives.\n",
    "$$ precision={{TP} \\over {TP + FP}}$$.\n",
    "\n",
    "This roughly translates to the classifier's \"confidence\" in predicting positives--important where False positives are very costly.\n",
    "\n",
    "### Recall\n",
    "<mark> Definition</mark>: Fraction of True positives to actual positives.\n",
    "$$Recall = {{TP} \\over {TP + FN}}$$\n",
    "\n",
    "Recall is crucial in applications where missing positive cases (False Negatives) has serious consequences: Medical diagnosis, Disaster alerts, etc.\n",
    "\n",
    "### F1-Score\n",
    "<mark>Definition</mark>: Harmonic mean of precision and recall.\n",
    "$$F1-score = 2*\\frac{Precision*Recall}{Precision+Recall}$$\n",
    "\n",
    "Precision focuses on the quality of positive predictions (How many of the predicted positives are correct?), whereas Recall focuses on the quantity of true positives identified (How many of the actual positives were found?).\n",
    "Together, they provide a fuller picture of a classifier's performance and are often combined using the F1-score.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9d9f87",
   "metadata": {},
   "source": [
    "Main Credits @ https://python-course.eu/machine-learning/evaluation-metrics.php and @ https://dl.acm.org/doi/10.3115/1073083.1073135 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
